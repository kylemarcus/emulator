#!/bin/sh
#SBATCH --partition=general-compute
#SBATCH --time=05:00:00
#SBATCH --nodes=20
#SBATCH --ntasks-per-node=2
#SBATCH --job-name="FINAL_REDUCE_Aug_28"
#SBATCH --output=FINAL_REDUCE_Aug_28
#SBATCH --mail-user=shivaswa@buffalo.edu
#SBATCH --mail-type=ALL
##SBATCH --requeue
#Specifies that the job will be requeued after a node failure.
#The default is that the job will not be requeued.

echo "SLURM_JOB_ID="$SLURM_JOB_ID
echo "SLURM_JOB_NODELIST"=$SLURM_JOB_NODELIST
echo "SLURM_NNODES"=$SLURM_NNODES
echo "SLURMTMPDIR="$SLURMTMPDIR
echo $SLURM_JOB_NODELIST | python node_list.py > MY_NODE_LIST




cd ~/my_hadoop
cat MY_NODE_LIST | ./clean_up.sh

./hadoop_initialiser.sh $SLURM_NNODES
sleep 10
#icpc -Wl,--start-group -I$MKL/include -L$MKL/lib/em64t -lmkl_sequential -lmkl_intel_lp64 -lmkl_core -Wl,--end-group -lpthread newEmulator.cpp -o newEmulator
#icpc -Wl,-rpath=$MKL/lib/em64t,--start-group -I$MKL/include -L$MKL/lib/em64t -lmkl_sequential -lmkl_intel_lp64 -lmkl_core -Wl,--end-group -lpthread newEmulator.cpp -o newEmulator


sleep 10

cd ~/hadoop-0.20.1
bin/hadoop dfs -copyFromLocal /panasas/scratch/shivaswa/phm_resample_mean_Aug_28/part* /

sleep 5

bin/hadoop jar contrib/streaming/hadoop-*streaming*.jar \
-D mapred.reduce.tasks=100  \
-file /user/shivaswa/my_hadoop/map6.py   -mapper map6.py \
-file  /user/shivaswa/my_hadoop/reduce5.py  -reducer reduce5.py \
-file  /user/shivaswa/my_hadoop/reduce3.py \
-file  /user/shivaswa/my_hadoop/neighbor.py \
-file  /user/shivaswa/my_hadoop/parser.py \
-file  /user/shivaswa/my_hadoop/build_data_set.py \
-input /part* -output /final_output


sleep 10


bin/hadoop dfs -copyToLocal /final_output/* /panasas/scratch/shivaswa/final_output_Aug_28

#sleep 6000

for node in `cat MY_NODE_LIST`
do
ssh $node 'bash -s' < copy_log.sh $node
done

cd ~/my_hadoop
./stop_hadoop.sh

